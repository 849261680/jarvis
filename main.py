# To run this code you need to install the following dependencies:
# pip install google-genai python-dotenv

import base64
import os
import sys
from dotenv import load_dotenv
from google import genai
from google.genai import types

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


def load_system_prompts():
    """ä» system_prompts æ–‡ä»¶å¤¹åŠ è½½æ‰€æœ‰ md æ–‡ä»¶"""
    prompts = {}
    prompt_dir = "system_prompts"

    if not os.path.exists(prompt_dir):
        print("âŒ æœªæ‰¾åˆ° system_prompts æ–‡ä»¶å¤¹")
        return {}

    try:
        # éå†æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰ md æ–‡ä»¶
        files = [f for f in os.listdir(prompt_dir) if f.endswith('.md')]

        for i, filename in enumerate(files, 1):
            # è·å–è§’è‰²åï¼ˆå»æ‰ .md åç¼€ï¼‰
            role_name = filename[:-3]

            # è¯»å–æ–‡ä»¶å†…å®¹
            with open(os.path.join(prompt_dir, filename), 'r', encoding='utf-8') as f:
                content = f.read().strip()

            prompts[str(i)] = {
                "name": role_name,
                "prompt": content
            }

        return prompts

    except Exception as e:
        print(f"âŒ åŠ è½½é…ç½®æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return {}


class ChatBot:
    def __init__(self, system_prompt=None, role_name="default"):
        self.client = genai.Client(
            api_key=os.environ.get("GEMINI_API_KEY"),
        )
        self.model = "gemini-2.5-pro"
        self.conversation_history = []
        self.system_instruction = system_prompt  # æ­£ç¡®å­˜å‚¨ç³»ç»ŸæŒ‡ä»¤
        self.role_name = role_name
        # åˆ›å»º chat_history æ–‡ä»¶å¤¹ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        if not os.path.exists("chat_history"):
            os.makedirs("chat_history")
        # ç®€å•çš„æ—¥å¿—æ–‡ä»¶åï¼Œä¿å­˜åœ¨ chat_history æ–‡ä»¶å¤¹ä¸­
        self.log_file = f"chat_history/chat_{role_name}.md"

    def save_to_file(self, role, text):
        """ç®€å•ä¿å­˜åˆ°æ–‡ä»¶"""
        with open(self.log_file, 'a', encoding='utf-8') as f:
            if role == "user":
                f.write(f"ç”¨æˆ·: {text}\n\n")
            else:
                f.write(f"{self.role_name}: {text}\n\n")

    def add_to_history(self, role, text):
        """æ·»åŠ æ¶ˆæ¯åˆ°å¯¹è¯å†å²"""
        self.conversation_history.append(
            types.Content(
                role=role,
                parts=[types.Part.from_text(text=text)]
            )
        )

    def generate_response(self, user_input):
        """ç”ŸæˆAIå“åº”"""
        # æ·»åŠ ç”¨æˆ·è¾“å…¥åˆ°å†å²å¹¶ä¿å­˜
        self.add_to_history("user", user_input)
        self.save_to_file("user", user_input)

        # é…ç½®ç”Ÿæˆå‚æ•°ï¼ˆæ­£ç¡®è®¾ç½®ç³»ç»ŸæŒ‡ä»¤ï¼‰
        generate_content_config = types.GenerateContentConfig(
            system_instruction=self.system_instruction,  # æ­£ç¡®çš„ç³»ç»ŸæŒ‡ä»¤è®¾ç½®æ–¹å¼
            thinking_config=types.ThinkingConfig(
                thinking_budget=200,
            ),
        )

        try:
            # ç”Ÿæˆæµå¼å“åº”
            response_text = ""
            for chunk in self.client.models.generate_content_stream(
                model=self.model,
                contents=self.conversation_history,
                config=generate_content_config,
            ):
                if chunk.text:
                    print(chunk.text, end="", flush=True)
                    response_text += chunk.text

            print()  # æ·»åŠ æ¢è¡Œç¬¦

            # æ·»åŠ AIå“åº”åˆ°å†å²å¹¶ä¿å­˜
            if response_text:
                self.add_to_history("model", response_text)
                self.save_to_file("model", response_text)

        except Exception as e:
            print(f"\né”™è¯¯: {e}")

    def chat(self):
        """å¼€å§‹å¯¹è¯å¾ªç¯"""
        print(f"ğŸ¤– {self.role_name} AI å·²å¯åŠ¨ï¼")
        print("ğŸ’¡ è¾“å…¥ 'quit', 'exit' æˆ– 'bye' æ¥é€€å‡ºå¯¹è¯\n")

        while True:
            try:
                user_input = input("ğŸ‘¤ ä½ : ").strip()

                # æ£€æŸ¥é€€å‡ºå‘½ä»¤
                if user_input.lower() in ['quit', 'exit', 'bye', 'é€€å‡º']:
                    print("ğŸ‘‹ å†è§ï¼")
                    break

                # æ£€æŸ¥ç©ºè¾“å…¥
                if not user_input:
                    print("è¯·è¾“å…¥ä¸€äº›å†…å®¹...")
                    continue

                print(f"ğŸ¤– {self.role_name}: ", end="")
                self.generate_response(user_input)
                print()

            except KeyboardInterrupt:
                print("\nğŸ‘‹ å†è§ï¼")
                break
            except EOFError:
                print("\nğŸ‘‹ å†è§ï¼")
                break


def main():
    # æ£€æŸ¥APIå¯†é’¥
    if not os.environ.get("GEMINI_API_KEY"):
        print("âŒ é”™è¯¯: è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® GEMINI_API_KEY")
        sys.exit(1)

    # åŠ è½½ç³»ç»Ÿæç¤ºè¯é…ç½®
    system_prompts = load_system_prompts()

    if not system_prompts:
        print("ä½¿ç”¨é»˜è®¤é…ç½®")
        system_prompts = {
            "1": {
                "name": "é»˜è®¤åŠ©æ‰‹",
                "prompt": "ä½ æ˜¯ä¸€ä¸ªå‹å–„çš„ AI åŠ©æ‰‹"
            }
        }

    print("ğŸ­ è¯·é€‰æ‹© AI è§’è‰²:")
    for key, value in system_prompts.items():
        print(f"{key}. {value['name']}")

    # è®¡ç®—ä¸‹ä¸€ä¸ªå¯ç”¨ç¼–å·ä½œä¸º"æ— ç³»ç»Ÿæç¤ºè¯"é€‰é¡¹
    max_num = max([int(k) for k in system_prompts.keys() if k.isdigit()], default=0)
    no_prompt_option = str(max_num + 1)
    print(f"{no_prompt_option}. æ— ç³»ç»Ÿæç¤ºè¯")
    print()

    choice = input("è¯·è¾“å…¥è§’è‰²ç¼–å·: ")

    if choice in system_prompts:
        role_info = system_prompts[choice]
        system_prompt = role_info["prompt"]
        role_name = role_info["name"]
    elif choice == no_prompt_option:
        system_prompt = None
        role_name = "æ— è§’è‰²"
    else:
        print("æ— æ•ˆé€‰æ‹©ï¼Œä½¿ç”¨é»˜è®¤è®¾ç½®")
        system_prompt = None
        role_name = "é»˜è®¤"

    # å¯åŠ¨èŠå¤©æœºå™¨äºº
    bot = ChatBot(system_prompt, role_name)
    bot.chat()


if __name__ == "__main__":
    main()